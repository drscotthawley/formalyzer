{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bac34fc",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    ">Here's where the main `formalyzer` workflow is defined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e54fc5",
   "metadata": {},
   "source": [
    "```markdown\n",
    "formalyzer: \n",
    "\n",
    "Reads PDF reccomendation letter, fills in admissions form(s)\n",
    "\n",
    "usage: \n",
    "  formalyzer <recc_letter.pdf> <url_list.txt>\n",
    "\n",
    "Instead of url_list.txt, a single URL can be given (esp. for testing purposes) \n",
    "\n",
    "Description: \n",
    "Formalyzer will scrape the text from the PDF recc letter, \n",
    "and for each URL in url_list, it will: \n",
    "- launch a browser tab for that url \n",
    "- fill in the form using what the LLM has gleaned from the recc letter\n",
    "- attach the PDF via the form's upload/attachment button\n",
    "...and do no more. \n",
    "The user will need to review the page and press the Submit button manually.\n",
    "\n",
    "\n",
    "Requirements: \n",
    "- Playwright \n",
    "- ANTHROPIC_API_KEY env var. (Could support other LLMs layer)\n",
    "- pypdf  \n",
    "\n",
    "Author: Scott H. Hawley, @drscotthawley\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac124dc7",
   "metadata": {
    "time_run": "2025-11-26T05:34:13.688716+00:00"
   },
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d00bc39",
   "metadata": {
    "time_run": "2025-11-26T05:34:14.784538+00:00"
   },
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76a1146",
   "metadata": {
    "time_run": "2025-11-26T05:34:15.552193+00:00"
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "import os \n",
    "\n",
    "def read_recc_info(info_file:str) -> list:\n",
    "    \"read a text file of info on the reviewer\" \n",
    "    with open(os.path.expanduser(info_file)) as f:\n",
    "        return f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc93a55d",
   "metadata": {
    "time_run": "2025-11-26T06:09:24.507513+00:00"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Reccomender Name: Scott H. Hawley \\nTitle: Professor of Physics \\n\\nAddress: \\nBelmont University \\n1900 Belmont Blvd \\nNashville, TN 37211\\n\\nPhone: 615-460-6206\\nEmail: scott.hawley@belmont.edu\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recc_info = read_recc_info(\"~/recc_info.txt\") \n",
    "recc_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3251b0d",
   "metadata": {
    "time_run": "2025-11-26T05:34:17.301970+00:00"
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "def read_urls_file(urls_file:str) -> list:\n",
    "    \"read a text file where each line is a url of a submission site\" \n",
    "    with open(os.path.expanduser(urls_file)) as f:\n",
    "        return f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cb7a57",
   "metadata": {
    "time_run": "2025-11-26T05:34:18.175761+00:00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 urls in list\n"
     ]
    }
   ],
   "source": [
    "urls = read_urls_file(\"~/recc_urls.txt\") \n",
    "print(f\"{len(urls)} urls in list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3d0c0a",
   "metadata": {
    "time_run": "2025-11-26T05:34:18.992071+00:00"
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "from pypdf import PdfReader\n",
    "import logging\n",
    "logging.getLogger(\"pypdf\").setLevel(logging.ERROR)\n",
    "\n",
    "def read_pdf_text(pdf_file):\n",
    "    reader = PdfReader(os.path.expanduser(pdf_file))\n",
    "    return \"\\n\".join(page.extract_text() for page in reader.pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4147ccb2",
   "metadata": {
    "time_run": "2025-11-26T05:34:20.098746+00:00"
   },
   "outputs": [],
   "source": [
    "letter_text = read_pdf_text(\"~/recc_letter.pdf\")\n",
    "#letter_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec63694d",
   "metadata": {
    "time_run": "2025-11-26T05:16:54.531384+00:00"
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "from bs4 import BeautifulSoup\n",
    "import json, re\n",
    "\n",
    "\n",
    "def scrape_form_fields(html):\n",
    "    \"\"\"Extract all fillable form fields from HTML\"\"\"\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    fields = []\n",
    "    for inp in soup.find_all(['input', 'select', 'textarea']):\n",
    "        field_id = inp.get('id', '')\n",
    "        if not field_id: continue\n",
    "        field_type = inp.get('type', inp.name)\n",
    "        if field_type in ['hidden', 'submit', 'button']: continue\n",
    "        \n",
    "        label = soup.find('label', {'for': field_id})\n",
    "        label_text = label.get_text(strip=True) if label else ''\n",
    "        current_value = inp.get('value', '')\n",
    "        \n",
    "        options = None\n",
    "        if inp.name == 'select':\n",
    "            options = [opt.get_text(strip=True) for opt in inp.find_all('option') if opt.get_text(strip=True)]\n",
    "        \n",
    "        fields.append({\n",
    "            'id': field_id, 'label': label_text, 'type': field_type,\n",
    "            'options': options, 'prefilled': bool(current_value and field_type not in ['radio','checkbox'] and inp.name != 'select')\n",
    "        })\n",
    "    return fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9577d05",
   "metadata": {
    "time_run": "2025-11-26T06:42:24.580515+00:00"
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "from claudette import Chat\n",
    "\n",
    "def get_field_mappings(fields, recc_info, letter_text, model=\"claude-sonnet-4-20250514\", debug=False):\n",
    "    \"\"\"Use LLM to map recommender info and letter to form fields\"\"\"\n",
    "    prompt = f\"\"\"You are filling out a graduate school recommendation form.\n",
    "\n",
    "RECOMMENDER INFO:\n",
    "{recc_info}\n",
    "\n",
    "RECOMMENDATION LETTER:\n",
    "{letter_text}\n",
    "\n",
    "FORM FIELDS TO FILL:\n",
    "{json.dumps([f for f in fields if not f['prefilled']], indent=2)}\n",
    "\n",
    "For each field, provide the field ID and value to fill. For dropdowns, pick from the options listed.\n",
    "Return as JSON array: [{{\"id\": \"form_xxx\", \"value\": \"...\"}}]\n",
    "Skip radio buttons.\n",
    "\"\"\"\n",
    "    chat = Chat(model=model)\n",
    "    if debug: print(f\"  Prompt length is {len(prompt)} characters\")\n",
    "    response = chat(prompt)\n",
    "    json_match = re.search(r'```json\\s*(.*?)\\s*```', response.content[0].text, re.DOTALL)\n",
    "    return json.loads(json_match.group(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ce76a9",
   "metadata": {
    "time_run": "2025-11-26T06:46:27.524239+00:00"
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "async def fill_form(page, mappings, skip_prefilled=True, debug=False):\n",
    "    \"\"\"Fill form fields using Playwright\"\"\"\n",
    "    results = {'filled': [], 'skipped': [], 'errors': []}\n",
    "    for i, item in enumerate(mappings):\n",
    "        field_id, value = item['id'], item['value']\n",
    "        if debug: print(f\"Mapping {i+1} of {len(mappings)}:  Processing {field_id}...\")\n",
    "        try:\n",
    "            elem = page.locator(f'#{field_id}')\n",
    "            tag = await elem.evaluate('el => el.tagName.toLowerCase()')\n",
    "            \n",
    "            if skip_prefilled and tag != 'select':\n",
    "                current = await elem.input_value()\n",
    "                if current:\n",
    "                    results['skipped'].append(field_id)\n",
    "                    continue\n",
    "            \n",
    "            if tag == 'select':\n",
    "                await elem.select_option(label=value)\n",
    "            else:\n",
    "                await elem.fill(value)\n",
    "            results['filled'].append(field_id)\n",
    "        except Exception as e:\n",
    "            results['errors'].append({'id': field_id, 'error': str(e)[:50]})\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b35db88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "async def upload_recommendation(page, file_path):\n",
    "    \"\"\"Upload the recommendation PDF\"\"\"\n",
    "    file_input = page.locator('input[type=\"file\"]').first\n",
    "    await file_input.set_input_files(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca22956",
   "metadata": {
    "time_run": "2025-11-26T06:46:37.830439+00:00"
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "import asyncio\n",
    "\n",
    "async def process_url(page, url, recc_info, letter_text, pdf_path, debug=False):\n",
    "    \"\"\"Process a single recommendation URL\"\"\"\n",
    "    await page.goto(url)\n",
    "    html = await page.content()\n",
    "    \n",
    "    if debug: print(\"Scraping form fields\")\n",
    "    fields = scrape_form_fields(html)\n",
    "    if debug: print(f\"Found {len(fields)} fields\")\n",
    "    \n",
    "    if debug: print(\"Calling LLM to get field mappings\")\n",
    "    mappings = get_field_mappings(fields, recc_info, letter_text, debug=debug)\n",
    "    if debug: print(f\"Got {len(mappings)} mappings from LLM\")\n",
    "    \n",
    "    if debug: print(\"Filling in form\")\n",
    "    results = await fill_form(page, mappings, debug=debug)\n",
    "    if debug: print(f\"Filled: {len(results['filled'])}, Errors: {len(results['errors'])}\")\n",
    "    \n",
    "    if debug: print(\"Uploading PDF\") \n",
    "    await upload_recommendation(page, pdf_path)\n",
    "    if debug: print(\"Uploaded PDF\")\n",
    "    \n",
    "    input(\"Review the form, then press Enter to continue to next URL (or Ctrl+C to stop)...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80c2345",
   "metadata": {},
   "source": [
    "# `formalyzer` CLI script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20aeb501",
   "metadata": {
    "time_run": "2025-11-26T05:31:28.369624+00:00"
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "def read_info(recc_info:str, pdf_path:str, urls:str):\n",
    "    \"parse CLI args and read input files\"\n",
    "    recc_info, pdf_path = [os.path.expanduser(_) for _ in [recc_info, pdf_path]]\n",
    "    assert os.path.exists(recc_info), f\"File not found: {recc_info}\"\n",
    "    assert os.path.exists(pdf_path), f\"File not found: {pdf_path}\"\n",
    "    recc_info = read_recc_info(recc_info) \n",
    "    letter_text = read_pdf_text(pdf_path)\n",
    "    if os.path.exists(os.path.expanduser(urls)): \n",
    "        print(f\"File {urls} exists. Reading.\")\n",
    "        urls = read_urls_file(urls)\n",
    "    else: \n",
    "        print(f\"No file {urls}. Treating it as a single url\") \n",
    "        urls = [urls]\n",
    "    return recc_info, letter_text, urls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95243e5",
   "metadata": {
    "time_run": "2025-11-26T05:52:23.109385+00:00"
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "import os \n",
    "from playwright.async_api import async_playwright\n",
    "from fastcore.script import call_parse\n",
    "\n",
    "async def setup_browser():\n",
    "    \"\"\"Connect to Chrome with remote debugging\"\"\"\n",
    "    pw = await async_playwright().start()\n",
    "    browser = await pw.chromium.connect_over_cdp(\"http://localhost:9222\")\n",
    "    page = await browser.new_page()\n",
    "    return pw, browser, page\n",
    "\n",
    "\n",
    "async def run_formalyzer(recc_info, letter_text, urls, pdf_path, debug=False):\n",
    "    \"\"\"Main async workflow\"\"\"\n",
    "    pw, browser, page = await setup_browser()\n",
    "    try:\n",
    "        for i, url in enumerate(urls):\n",
    "            if not url.strip(): continue  # skip empty urls\n",
    "            print(f\"\\nURL {i+1} of {len(urls)}: {url}\")\n",
    "            await process_url(page, url, recc_info, letter_text, pdf_path, debug=debug)\n",
    "    finally:\n",
    "        await browser.close()\n",
    "        await pw.stop()\n",
    "\n",
    "\n",
    "@call_parse\n",
    "def main(recc_info:str, pdf_path:str, urls:str, debug:bool=False):\n",
    "    assert os.environ.get('ANTHROPIC_API_KEY'), \"Please set ANTHROPIC_API_KEY environment variable\" # used by Claudette\n",
    "    recc_info, letter_text, urls = read_info(recc_info, pdf_path, urls)\n",
    "    if debug:\n",
    "        print(\"recc_info =\\n\", recc_info)\n",
    "        print(\"letter_text =\\n\", letter_text)\n",
    "        print(\"urls =\\n\", urls)\n",
    "    \n",
    "    # Run the async workflow\n",
    "    asyncio.run(run_formalyzer(recc_info, letter_text, urls, pdf_path, debug))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161cceb3",
   "metadata": {
    "time_run": "2025-11-26T05:32:55.294919+00:00"
   },
   "outputs": [],
   "source": [
    "#main(\"~/recc_info.txt\", \"~/recc_letter.pdf\",\"~/recc_urls.txt\", debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e4d445",
   "metadata": {
    "time_run": "2025-11-26T05:55:34.535321+00:00"
   },
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c8d940",
   "metadata": {
    "solveit_ai": true,
    "use_thinking": true
   },
   "source": [
    "I have debugging turned on, and i see no status updates after \"Filling in form\": \n",
    "```\n",
    "URL 1 of 11: https://apply.grad.ucla.edu/apply/refer?key=0229711527540226\n",
    "Scraping form fields\n",
    "Found 128 fields\n",
    "Calling LLM to get field mappings\n",
    "Got 44 mappings from LLM\n",
    "Filling in form\n",
    "```\n",
    "I'd expect it to say something about \"Filled:\". Looking at the form, I see that it's been filled out. I guess it's just somehow that the await Phil form isn't returning. What do you think's wrong? \n",
    "\n",
    "##### ğŸ¤–ReplyğŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ \n",
    "\n",
    "Since the form appears filled but `fill_form` isn't returning, it's likely hanging on one specific field operation â€” probably the last one or one that's causing a Playwright timeout.\n",
    "\n",
    "A quick way to diagnose: add a print inside the loop in `fill_form` to see which field it's processing. Something like:\n",
    "\n",
    "```python\n",
    "print(f\"  Processing {field_id}...\")\n",
    "```\n",
    "\n",
    "right after the `field_id, value = ...` line. That way you'll see exactly where it stalls.\n",
    "\n",
    "Does that make sense as a first step?"
   ]
  }
 ],
 "metadata": {
  "solveit_dialog_mode": "learning",
  "solveit_ver": 2
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
